[![dep1](https://img.shields.io/badge/Python-3.7.3-brightgreen.svg)](https://www.python.org/)
[![dep1](https://img.shields.io/badge/Tensorflow-2.1-brightgreen.svg)](https://www.tensorflow.org/)
[![dep2](https://img.shields.io/badge/Keras-2.2.4-brightgreen.svg)](https://keras.io/)


# Encoder-Decoder-models

The file provide several alternative ways of specifying encoder-decoder model using Keras and Tensorflow. The examples include:  

- Teacher forcing approach - general example (LSTM and GRU)
- Teacher forcing approach - translation example with embedding layer
- Encoder-decoder models without teacher forcing (based on RepeatVector)
- Stateful encoder-decoder model (based on RepeatVector)


## References:

- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
- https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/
- https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/
